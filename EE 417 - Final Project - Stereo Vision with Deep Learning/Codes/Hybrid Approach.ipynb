{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOozZrzRx5N5JyCo3D5FJvc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"VNHB_myAkFbP","executionInfo":{"status":"ok","timestamp":1748950423719,"user_tz":-180,"elapsed":17183,"user":{"displayName":"Barış Bakırdöven (Student)","userId":"09815592257537213217"}}},"outputs":[],"source":["#!/usr/bin/env python3\n","\"\"\"\n","HYBRID + CLASSICAL STEREO PIPELINE  ───────────────────────────────────────\n","Author  : Barış Bakırdöven\n","Date    : 2025‑06‑03\n","\n","This single script offers two complementary depth‑estimation workflows on\n","KITTI Stereo 2015:\n","\n","  1. **Hybrid** – A lightweight U‑Net encoder is first *learned* using KITTI\n","     disparity supervision; at inference its frozen feature maps are fed\n","     to a hand‑engineered block‑matching stage.\n","  2. **Classical SGM** – Standard Semi‑Global Matching (OpenCV StereoSGBM)\n","     runs directly on grey‑scale images, mirroring the earlier MATLAB\n","     implementation (median filtering, KITTI D1‑all evaluation, assorted\n","     visualisations).\n","\n","Invocation examples\n","────────────────────\n","$ python hybrid.py                           # train extractor + hybrid eval\n","$ python hybrid.py --no‑train --weights ext.h5   # hybrid eval only\n","$ python hybrid.py --classical                  # classical SGM only\n","\n","All outputs (weights, metrics, PNGs) appear under ./outputs_hybrid/.\n","\"\"\"\n","\n","# ───────────────────────── Imports & globals ────────────────────────────\n","import os, glob, time, pathlib, argparse\n","from typing import Dict, Tuple\n","\n","import numpy as np\n","import cv2                                    # ← classical SGM\n","import tensorflow as tf\n","from tensorflow.keras import layers, Model\n","\n","import matplotlib.pyplot as plt; import matplotlib as mpl\n","mpl.rcParams[\"figure.autolayout\"] = True\n","\n","\n","# Base directory where both stereo images and calib files sit\n","BASE_KITTI = pathlib.Path(\n","    r\"C:\\Users\\Baris\\Downloads\\ee417\\kitti\"\n",")\n","\n","TRAIN_LEFT_DIR  = BASE_KITTI / \"data_scene_flow\" / \"training\" / \"image_2\"\n","TRAIN_RIGHT_DIR = BASE_KITTI / \"data_scene_flow\" / \"training\" / \"image_3\"\n","TRAIN_GT_DIR    = BASE_KITTI / \"data_scene_flow\" / \"training\" / \"disp_occ_0\"\n","\n","TEST_LEFT_DIR   = BASE_KITTI / \"data_scene_flow\" / \"testing\" / \"image_2\"\n","TEST_RIGHT_DIR  = BASE_KITTI / \"data_scene_flow\" / \"testing\" / \"image_3\"\n","CALIB_DIR = BASE_KITTI / \"data_scene_flow_calib\" / \"training\" / \"calib_cam_to_cam\"\n","\n","SAVE_DIR          = pathlib.Path(\"./outputs_hybrid\"); SAVE_DIR.mkdir(exist_ok=True)\n","\n","TARGET_SIZE  = (384, 1248)      # H×W after cropping sky rows\n","VAL_SPLIT    = 0.2\n","BATCH_SIZE   = 2\n","MAX_DISP     = 192; MAX_DISP_Q = MAX_DISP//4\n","EPOCHS_MAIN, EPOCHS_FINE = 5, 2\n","LR_MAIN,    LR_FINE      = 1e-3, 1e-4\n","N_SHOW = 2\n","\n","# ──────────────────────── Utility I/O helpers ───────────────────────────\n","\n","def list_png(folder: pathlib.Path) -> Dict[str,str]:\n","    return {os.path.basename(p): p for p in glob.glob(str(folder/\"*.png\"))}\n","\n","\n","def load_rgb(fp: tf.Tensor) -> tf.Tensor:\n","    img = tf.image.decode_png(tf.io.read_file(fp), channels=3)\n","    return tf.image.convert_image_dtype(img, tf.float32)\n","\n","\n","def load_disp(fp: tf.Tensor) -> tf.Tensor:\n","    raw = tf.image.decode_png(tf.io.read_file(fp), channels=1, dtype=tf.uint16)\n","    return tf.cast(raw, tf.float32) / 256.0\n","\n","# ───────────────────────── tf.data pipelines ────────────────────────────\n","\n","def make_dataset(keys, L_map, R_map, G_map):\n","    LP = tf.constant([L_map[k] for k in keys]); RP = tf.constant([R_map[k] for k in keys]); GP = tf.constant([G_map[k] for k in keys])\n","    def _f(lp,rp,gp):\n","        L = tf.image.resize(load_rgb(lp), TARGET_SIZE); R = tf.image.resize(load_rgb(rp), TARGET_SIZE)\n","        D = tf.image.resize(load_disp(gp), TARGET_SIZE, method='nearest');\n","        return (L,R), D\n","    return (tf.data.Dataset.from_tensor_slices((LP,RP,GP)).map(_f, tf.data.AUTOTUNE)\n","            .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n","\n","def make_test_dataset(keys,L_map,R_map):\n","    LP = tf.constant([L_map[k] for k in keys]); RP = tf.constant([R_map[k] for k in keys])\n","    def _f(lp,rp):\n","        L=tf.image.resize(load_rgb(lp),TARGET_SIZE); R=tf.image.resize(load_rgb(rp),TARGET_SIZE); return (L,R)\n","    return (tf.data.Dataset.from_tensor_slices((LP,RP)).map(_f,tf.data.AUTOTUNE)\n","            .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n","\n","# ──────────────────────── Train/val/test splits ─────────────────────────\n","\n","def build_splits():\n","    L_ALL,R_ALL,G_ALL = map(list_png,(TRAIN_LEFT_DIR,TRAIN_RIGHT_DIR,TRAIN_GT_DIR))\n","    keys=[k for k in L_ALL if k in R_ALL and k in G_ALL]; keys.sort()\n","    from sklearn.model_selection import train_test_split\n","    tr,va = train_test_split(keys,test_size=VAL_SPLIT,random_state=42)\n","    train_ds=make_dataset(tr,L_ALL,R_ALL,G_ALL); val_ds=make_dataset(va,L_ALL,R_ALL,G_ALL)\n","    # --- testing (no GT) ---\n","    TL,TR=map(list_png,(TEST_LEFT_DIR,TEST_RIGHT_DIR)); tkeys=[k for k in TL if k in TR]; tkeys.sort()\n","    test_ds=make_test_dataset(tkeys,TL,TR)\n","    print(f\"→ train={len(tr)}  val={len(va)}  test={len(tkeys)}\")\n","    return train_ds,val_ds,test_ds\n","\n","# ─────────────────── Lightweight U‑Net encoder (¼‑res) ──────────────────\n","\n","def conv_bn_relu(x,ch,k=3,s=1):\n","    x=layers.Conv2D(ch,k,s,'same',use_bias=False)(x); x=layers.BatchNormalization()(x); return layers.ReLU()(x)\n","\n","def unet_encoder(backbone_ch=32, depth=2):\n","    inp=layers.Input((None,None,3)); x=inp\n","    for d in range(depth):\n","        for _ in range(2): x=conv_bn_relu(x, backbone_ch*(2**d))\n","        x=layers.MaxPool2D(2)(x)\n","    out=conv_bn_relu(x, backbone_ch*(2**depth))\n","    return Model(inp,out,name=\"UNetEncoder\")\n","\n","# ───────────────────── FeatureNet (training surrogate) ──────────────────\n","\n","def cost_volume(FL,FR,D=MAX_DISP_Q):\n","    cost=[]\n","    for d in range(D):\n","        slice_r=FR if d==0 else tf.pad(FR[:,:,:-d,:],[[0,0],[0,0],[d,0],[0,0]])\n","        cost.append(tf.reduce_mean(FL*slice_r,-1,keepdims=True))\n","    return tf.concat(cost,-1)\n","\n","class FeatureNet(Model):\n","    def __init__(self):\n","        super().__init__(); self.max_disp_q=MAX_DISP_Q; self.feat=unet_encoder()\n","    def call(self,inputs):\n","        L,R=inputs; FL,FR=self.feat(L),self.feat(R)\n","        cv=cost_volume(FL,FR,self.max_disp_q); logits=tf.expand_dims(cv,-1)\n","        prob=tf.nn.softmax(logits,axis=-2)[...,0]\n","        disp_q=tf.reduce_sum(prob*tf.range(self.max_disp_q,dtype=tf.float32),-1)\n","        return tf.image.resize(disp_q[...,None]*4.0, tf.shape(L)[1:3])  # full‑res\n","\n","# ─────────────────── KITTI metrics & masked MAE loss ───────────────────\n","\n","def masked_mae(gt,pred):\n","    m=tf.cast(gt>0,tf.float32); return tf.reduce_sum(tf.abs(gt-pred)*m)/tf.reduce_sum(m)\n","\n","def kitti_d1(pred,gt):\n","    m=tf.cast(gt>0,gt.dtype); ae=tf.abs(pred-gt); rel=ae/tf.maximum(gt,1e-6); bad=tf.logical_and(ae>3.0,rel>0.05)\n","    return tf.reduce_sum(tf.cast(bad,gt.dtype)*m), tf.reduce_sum(m)\n","\n","def mae_rmse(pred,gt):\n","    m=tf.cast(gt>0,gt.dtype); ae=tf.reduce_sum(tf.abs(pred-gt)*m); se=tf.reduce_sum(tf.square(pred-gt)*m); v=tf.reduce_sum(m)\n","    return ae,se,v\n","\n","# ─────────────────────────── HYBRID inference ──────────────────────────\n","\n","def block_match_feats(FL:np.ndarray,FR:np.ndarray,D=MAX_DISP_Q)->np.ndarray:\n","    H,W,_=FL.shape; cost=np.full((H,W,D),np.inf,np.float32)\n","    for d in range(D):\n","        slice_r=FR if d==0 else np.pad(FR[:,:-d,:],((0,0),(d,0),(0,0)),mode='constant')\n","        cost[:,:,d]=np.mean(np.abs(FL-slice_r),-1)\n","    return np.argmin(cost,-1).astype(np.float32)  # ¼‑res\n","\n","def hybrid_disparity(extractor:Model,L:tf.Tensor,R:tf.Tensor)->tf.Tensor:\n","    FL=extractor(L,training=False).numpy(); FR=extractor(R,training=False).numpy(); outs=[]\n","    for b in range(FL.shape[0]):\n","        disp_q=block_match_feats(FL[b],FR[b]); outs.append(tf.image.resize(disp_q[...,None]*4.0, tf.shape(L)[1:3]))\n","    return tf.concat(outs,0)\n","\n","# ───────────────────────── Classical SGM pipeline ───────────────────────\n","\n","def disparity_sgm_cv2(L_rgb:np.ndarray,R_rgb:np.ndarray, max_disp=128, uniqueness=15):\n","    grayL=cv2.cvtColor(L_rgb,cv2.COLOR_RGB2GRAY); grayR=cv2.cvtColor(R_rgb,cv2.COLOR_RGB2GRAY)\n","    # StereoSGBM requires numDisparities divisible by 16\n","    num_disp=(max_disp//16)*16\n","    sgbm=cv2.StereoSGBM_create(minDisparity=0, numDisparities=num_disp, blockSize=5,\n","                               P1=8*3*3**2, P2=32*3*3**2,\n","                               uniquenessRatio=uniqueness, speckleWindowSize=50,\n","                               speckleRange=2, disp12MaxDiff=1)\n","    disp=sgbm.compute(grayL,grayR).astype(np.float32)/16.0\n","    disp=cv2.medianBlur(disp,3)\n","    disp[disp<0]=0\n","    return disp\n","\n","# ─────────────── Evaluate pipeline (hybrid *or* classical) ──────────────\n","\n","def eval_loop(disp_fn, dataset, extractor=None):\n","    total_ae=total_se=total_bad=total_v=0.0; n=0; t0=time.perf_counter()\n","    for batch in dataset:\n","        if extractor: (L,R),G=batch; P=disp_fn(extractor,L,R)[...,0]; G=G[...,0]\n","        else: (L,R),G=batch; P=[];  # placeholder handled below\n","        if extractor is None:\n","            # loop over batch for classical SGM (CPU)\n","            for b in range(L.shape[0]):\n","                d=disparity_sgm_cv2((L[b].numpy()*255).astype(np.uint8), (R[b].numpy()*255).astype(np.uint8))\n","                P.append(cv2.resize(d,(TARGET_SIZE[1],TARGET_SIZE[0]),interpolation=cv2.INTER_LINEAR))\n","            P=np.stack(P,0)\n","        ae,se,v=mae_rmse(P,G); bad,_=kitti_d1(P,G)\n","        total_ae+=ae.numpy(); total_se+=se.numpy(); total_bad+=bad.numpy(); total_v+=v.numpy(); n+=int(L.shape[0])\n","    sec=(time.perf_counter()-t0)/n; mae=total_ae/total_v; rmse=np.sqrt(total_se/total_v); d1=total_bad/total_v*100\n","    return sec,d1,mae,rmse\n","\n","# ───────────────────────────── Training loop ────────────────────────────\n","\n","def train_extractor(train_ds,val_ds):\n","    fn=FeatureNet(); _=fn((tf.zeros((1,*TARGET_SIZE,3)),)*2)\n","    fn.compile(tf.keras.optimizers.Adam(LR_MAIN),loss=masked_mae); fn.fit(train_ds,validation_data=val_ds,epochs=EPOCHS_MAIN,verbose=1)\n","    if EPOCHS_FINE: fn.compile(tf.keras.optimizers.Adam(LR_FINE),loss=masked_mae); fn.fit(train_ds,validation_data=val_ds,epochs=EPOCHS_FINE,verbose=1)\n","    extractor=fn.feat; extractor.trainable=False; return extractor\n","\n","\n"]},{"cell_type":"code","source":["# ────────────────────────── Notebook-friendly MAIN ──────────────────────────\n","# Adjust these flags directly in the cell instead of using argparse\n","CLASSICAL_ONLY = False   # True → run only classical SGM\n","NO_TRAIN       = False   # True → skip training and load weights below\n","WEIGHTS_PATH   = \"\"      # Path to .h5 extractor weights (used if NO_TRAIN)\n","\n","# 1. Build datasets\n","train_ds, val_ds, _ = build_splits()\n","\n","# 2. Classical-only branch\n","if CLASSICAL_ONLY:\n","    sec, d1, mae, rmse = eval_loop(disparity_sgm_cv2, val_ds)\n","    print(f\"[CLASSICAL SGM] time/pair = {sec:.2f}s   D1 = {d1:.2f}%   \"\n","          f\"MAE = {mae:.2f}px   RMSE = {rmse:.2f}px\")\n","\n","# 3. Hybrid branch\n","else:\n","    if NO_TRAIN and WEIGHTS_PATH:\n","        extractor = unet_encoder()            # build backbone\n","        extractor.load_weights(WEIGHTS_PATH)  # load frozen weights\n","        extractor.trainable = False\n","    else:\n","        extractor = train_extractor(train_ds, val_ds)\n","        extractor.save_weights(SAVE_DIR / \"extractor_final.h5\")\n","\n","    sec, d1, mae, rmse = eval_loop(hybrid_disparity, val_ds, extractor)\n","    print(f\"[HYBRID] time/pair = {sec:.2f}s   D1 = {d1:.2f}%   \"\n","          f\"MAE = {mae:.2f}px   RMSE = {rmse:.2f}px\")\n","\n","    # 4. Quick qualitative dump (first N_SHOW validation batches)\n","    disp_cmap = mpl.colormaps['turbo'].copy()\n","    disp_cmap.set_bad('#303030')\n","\n","    for bid, ((L, R), G) in enumerate(val_ds.take(N_SHOW)):\n","        P = hybrid_disparity(extractor, L, R)[..., 0].numpy()\n","        for i in range(L.shape[0]):\n","            gt   = np.where(G[i, ..., 0] > 0, G[i, ..., 0], np.nan)\n","            pred = P[i]\n","            lo, hi = (0, MAX_DISP) if not np.isfinite(gt).any() \\\n","                     else np.nanpercentile(gt, [1, 99])\n","            norm = mpl.colors.Normalize(vmin=lo, vmax=hi)\n","\n","            fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n","            ax[0].imshow(L[i]);  ax[0].axis('off'); ax[0].set_title('Left')\n","            ax[1].imshow(gt, cmap=disp_cmap, norm=norm)\n","            ax[1].axis('off');   ax[1].set_title('GT')\n","            im = ax[2].imshow(pred, cmap=disp_cmap, norm=norm)\n","            ax[2].axis('off');   ax[2].set_title('Hybrid')\n","            fig.colorbar(im, ax=ax[1:], fraction=0.03, pad=0.02, label='px')\n","\n","            # Save & show\n","            out = SAVE_DIR / f\"hybrid_val_{bid}_{i}.png\"\n","            fig.savefig(out, dpi=150)\n","            plt.show()\n","            plt.close(fig)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"B0vSXYKCkKyT","executionInfo":{"status":"error","timestamp":1748950437832,"user_tz":-180,"elapsed":494,"user":{"displayName":"Barış Bakırdöven (Student)","userId":"09815592257537213217"}},"outputId":"9fb1ef5a-1a57-4b71-ebc6-abdce843b961"},"execution_count":2,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c0a1701ea82f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. Build datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 2. Classical-only branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-d712837fc49a>\u001b[0m in \u001b[0;36mbuild_splits\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL_ALL\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR_ALL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG_ALL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVAL_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL_ALL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR_ALL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_ALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL_ALL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR_ALL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_ALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# --- testing (no GT) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."]}]}]}